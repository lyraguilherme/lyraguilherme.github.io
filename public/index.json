[{"content":"The Machine Learning Workflow 1. The \u0026ldquo;80/20\u0026rdquo; Rule of ML This is a key concept that states that 80% of an AI project is spent on Data Preparation and only 20% is spent on actual model training.\nGIGO (Garbage In, Garbage Out): The quality of your model is strictly limited by the quality of your data. Sequential Process: Data must be collected, cleaned, and engineered before the model can begin the learning phase. 2. Data Collection \u0026amp; Integration Gathering raw data from scattered sources into a centralized Data Lake or \u0026ldquo;source of truth\u0026rdquo;.\nThe Process: Fetching logs, database records, or real-time streams (often referred to as ETL: Extract, Transform, Load). AWS Services to Remember:\nAmazon S3: Go-to storage for unstructured data (photos, raw logs). Amazon Kinesis: Used for ingesting real-time, high-speed data streams. AWS Glue: A serverless service for crawling, discovering, and cataloging data from various sources. 3. Data Preprocessing Data Preprocessing is the of converting \u0026ldquo;messy\u0026rdquo; raw data into a clean, standardized format that algorithms can process efficiently.\nCommon Tasks:\nCleaning: Removing duplicate entries and fixing structural errors. Imputation: Handling missing values by filling gaps (e.g., using the mean/median) or dropping incomplete rows. Scaling/Normalization: Adjusting numerical values to a common range (e.g., 0 to 1) so large numbers don\u0026rsquo;t disproportionately influence the model. AWS Tool: AWS Glue DataBrew (A visual, no-code tool for cleaning and normalizing data with 250+ built-in transformations). 4. Feature Engineering Feature Engineering is the \u0026ldquo;art\u0026rdquo; of selecting, creating, or modifying variables (features) to maximize the model\u0026rsquo;s predictive performance.\nKey Techniques:\nSelection: Keeping relevant columns (e.g., \u0026ldquo;Price\u0026rdquo;) but dropping useless ones (e.g., \u0026ldquo;Internal ID\u0026rdquo;). Creation: Combining existing data to make new insights (e.g., turning \u0026ldquo;Date\u0026rdquo; and \u0026ldquo;Time\u0026rdquo; into \u0026ldquo;Is_Holiday\u0026rdquo;). Transformation/Encoding: Converting text categories (e.g., \u0026ldquo;High\u0026rdquo;, \u0026ldquo;Low\u0026rdquo;) into numerical values (e.g., 1, 0) via techniques like One-Hot Encoding. AWS Tool: Amazon SageMaker Data Wrangler (The primary tool for preparing features specifically for ML within SageMaker). 5. Data Visualization Representing data graphically to identify patterns, trends, and anomalies before training.\nEDA (Exploratory Data Analysis): The practice of spotting errors, biases, or data imbalances early in the lifecycle. Correlation: Identifying how variables relate to one another (e.g., \u0026ldquo;Temperature\u0026rdquo; vs. \u0026ldquo;Ice Cream Sales\u0026rdquo;). AWS Tool: Amazon QuickSight (The primary cloud-native BI service used for creating dashboards and visualizing ML insights). 6. Training, Evaluation \u0026amp; Deployment The \u0026ldquo;After\u0026rdquo; Stages: Once data is prepped, the model-building phase begins. Model Training: The algorithm analyzes the prepared data to find mathematical patterns and weights. Evaluation: Testing the model against a \u0026ldquo;Validation Dataset\u0026rdquo; to ensure it hasn\u0026rsquo;t Overfit (memorized the data instead of learning). Deployment: Hosting the model on an endpoint so applications can make real-world predictions. AWS Tool: Amazon SageMaker (The comprehensive, end-to-end platform for the entire ML lifecycle). ","permalink":"http://localhost:1313/posts/aws-ai-practitioner-pt1/","summary":"\u003ch1 id=\"the-machine-learning-workflow\"\u003eThe Machine Learning Workflow\u003c/h1\u003e\n\u003ch2 id=\"1-the-8020-rule-of-ml\"\u003e1. The \u0026ldquo;80/20\u0026rdquo; Rule of ML\u003c/h2\u003e\n\u003cp\u003eThis is a key concept that states that 80% of an AI project is spent on \u003cstrong\u003eData Preparation\u003c/strong\u003e and only 20% is spent on actual model training.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eGIGO (Garbage In, Garbage Out):\u003c/strong\u003e The quality of your model is strictly limited by the quality of your data.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSequential Process:\u003c/strong\u003e Data must be collected, cleaned, and engineered \u003cem\u003ebefore\u003c/em\u003e the model can begin the learning phase.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"2-data-collection--integration\"\u003e2. Data Collection \u0026amp; Integration\u003c/h2\u003e\n\u003cp\u003eGathering raw data from scattered sources into a centralized \u003cstrong\u003eData Lake\u003c/strong\u003e or \u0026ldquo;source of truth\u0026rdquo;.\u003c/p\u003e","title":"AWS AI Practitioner - Study Notes #1"},{"content":"When you\u0026rsquo;re handed a Fortigate backup config and need to document dozens of IPsec VPNs ‚Äî but have no access to the device ‚Äî manual parsing becomes a pain.\nTo solve that, I built a Python script that automates the process.\nNo Device Access Needed ü•∑ This script works entirely offline. All it needs is a raw FortiGate config file (e.g., from show full-config) ‚Äî no API or CLI access required.\nAI-Assisted Parsing ü§ñ Some of the trickiest parts were handling nested edit/set/next blocks and mapping address groups to actual subnets. AI helped speed up the development and sanity-check the parsing logic.\nWhat It Does üîç Parses phase1-interface and phase2-interface IPsec VPN definitions Resolves source/destination address groups Maps all referenced address objects to subnets Displays the result in a clean, console-based table Exports structured output as a dictionary to a .txt file Output Example üì¶ The output clearly shows which local/remote subnets are used in each VPN ‚Äî perfect for audits, migrations, or documentation.\nUsing the script The full code is available on my GitHub: https://github.com/lyraguilherme/networkscripts/tree/main/fortigate_vpn_parser\nInstalling requirements Make sure you\u0026rsquo;re running Python 3.7+ Install Rich library pip install rich Running the script Save the full Fortigate config in the same folder as the script Run the script using the example syntax below Usage:\npython fg_vpn_parser.py \u0026lt;path_to_config.txt\u0026gt; ","permalink":"http://localhost:1313/posts/fortigate-vpn-parser/","summary":"\u003cp\u003eWhen you\u0026rsquo;re handed a Fortigate backup config and need to document dozens of IPsec VPNs ‚Äî but have no access to the device ‚Äî manual parsing becomes a pain.\u003c/p\u003e\n\u003cp\u003eTo solve that, I built a Python script that automates the process.\u003c/p\u003e\n\u003ch1 id=\"no-device-access-needed-\"\u003eNo Device Access Needed ü•∑\u003c/h1\u003e\n\u003cp\u003eThis script works entirely offline. All it needs is a raw FortiGate config file (e.g., from show full-config) ‚Äî no API or CLI access required.\u003c/p\u003e","title":"Fortigate VPN Parser"},{"content":"Honored to be selected as a Cisco Insider Champion for the second year in a row!\nAccording to Cisco:\nMembers of the Cisco Champion 2025 program have demonstrated, through a comprehensive application process, the propensity to influence the technical community through social engagement and/or content creation. Cisco Champions have access to ongoing and exclusive engagement with Cisco experts and technology.\nMy badge URL: https://www.credly.com/badges/50f57a9c-7f43-49d6-b9d1-de3706c4f10f/public_url\n","permalink":"http://localhost:1313/posts/cisco-insider-champion-2025/","summary":"\u003cp\u003eHonored to be selected as a Cisco Insider Champion for the second year in a row!\u003c/p\u003e\n\u003cp\u003eAccording to Cisco:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eMembers of the Cisco Champion 2025 program have demonstrated, through a comprehensive application process, the propensity to influence the technical community through social engagement and/or content creation. Cisco Champions have access to ongoing and exclusive engagement with Cisco experts and technology.\u003c/p\u003e\u003c/blockquote\u003e\n\u003cp\u003eMy badge URL: \u003ca href=\"https://www.credly.com/badges/50f57a9c-7f43-49d6-b9d1-de3706c4f10f/public_url\"\u003ehttps://www.credly.com/badges/50f57a9c-7f43-49d6-b9d1-de3706c4f10f/public_url\u003c/a\u003e\u003c/p\u003e","title":"Cisco Insider Champion 2025"},{"content":"We are back with another exciting episode of Routing Friends! This time, we are going to talk about network automation and work in a practical laboratory with specific tools/frameworks to carry out operations with automation!\nLink to YouTube (content in PT-BR): Desvendando a Automa√ß√£o de Redes: Lab Hands-On com Guilherme Lyra, CCIE #66666 | Epis√≥dio 167 Laboratory Use Guide 1. Preparing your automation host The first step is to organize your own automation host, which is nothing more than the computer where you will execute the scripts. The only point that should be noted is that this computer requires access to your laboratory\u0026rsquo;s network equipment.\nI recommend using a Linux distribution for this host, but it can be used on Mac or Windows as well. I personally like to use a Debian VM for this.\nApplications required on your automation host:\nPython Ansible 2. Access to my example scripts I suggest you to clone the GitHub repository below. There you will find the topology and the scripts that were used during the podcast Routing Friends - Lab Hands-on carried out on November 21, 2024:\ngit clone https://github.com/lyraguilherme/LabHandsOn-Nov2024/ 3. Preparing or selling dependencies To use the scripts, you should preferably create a venv:\npython -m venv venv Activate the venv:\nsource venv/bin/activate Install the dependencies:\npip install -r requirements.txt 4. Setting credentials as environment variables After installing all the dependencies, define your network device\u0026rsquo;s credentials as environment variables:\nexport LAB_USERNAME=\u0026#34;my_username\u0026#34; export LAB_PASSWORD=\u0026#34;my_password\u0026#34; From now on, you will be able to use the scripts that I shared.\n","permalink":"http://localhost:1313/posts/routing-friends-automation-handson/","summary":"\u003cp\u003eWe are back with another exciting episode of Routing Friends! This time, we are going to talk about network automation and work in a practical laboratory with specific tools/frameworks to carry out operations with automation!\u003c/p\u003e\n\u003cp\u003eLink to YouTube (content in PT-BR):\n\u003ca href=\"https://www.youtube.com/watch?v=V8hF8toSAJ4\"\u003eDesvendando a Automa√ß√£o de Redes: Lab Hands-On com Guilherme Lyra, CCIE #66666 | Epis√≥dio 167 \u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"laboratory-use-guide\"\u003eLaboratory Use Guide\u003c/h2\u003e\n\u003ch3 id=\"1-preparing-your-automation-host\"\u003e1. Preparing your automation host\u003c/h3\u003e\n\u003cp\u003eThe first step is to organize your own automation host, which is nothing more than the computer where you will execute the scripts. The only point that should be noted is that this computer requires access to your laboratory\u0026rsquo;s network equipment.\u003c/p\u003e","title":"Routing Friends - Network Automation"},{"content":"Introduction In this post, I\u0026rsquo;ll walk you through how to use Terraform to set up a VPN site-to-site connection on AWS, leveraging Infrastructure as Code (IaC) to make the process quicker, easier, and fully repeatable.\nFor the examples below, we‚Äôll build a cloud infrastructure using a Virtual Private Gateway (VGW). In a future post, we‚Äôll explore using a Transit Gateway (TGW) for more complex setups.\nI‚Äôm running everything on a MacBook, but you can easily replicate these steps on a Linux jump host or any similar environment.\nPreparation AWS Identity and Access Management (IAM) If you don\u0026rsquo;t have one yet, the first step is to create an Access Key. This will allow you to authenticate and interact with AWS services through the CLI.\nFor this example, we‚Äôll create a restrictive policy that only grants access to the specific resources needed, following the principle of least privilege.\nAccess the AWS console and go to IAM. Under Access Management, go to Policies and click Create Policy. I named my policy custom_policy_vpn_only In the policy editor, select JSON and copy/paste the JSON below: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:DeleteVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:DescribeVpnGateways\u0026#34;, \u0026#34;ec2:DescribeCustomerGateways\u0026#34;, \u0026#34;ec2:DescribeVpnConnections\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:DetachVpnGateway\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Go back to the Access management menu, go to Users and click Create user Define the username as you wish. I created a user named \u0026ldquo;networkops\u0026rdquo;\u0026quot; On Set permissions, select Attach policies directly and select the policy we created before (use the search to make it easier to find the desired policy) Once the user is created, click on the username and on the Summary page, click \u0026ldquo;Create access key\u0026rdquo; For the Use case, select \u0026ldquo;Other\u0026rdquo; Save your access key and secret access key\nInstalling AWS CLI The first step is to install AWS CLI.\nThe procedure described below is available here: AWS Docs\nDownload the file using the curl command. The -o option specifies the file name that the downloaded package is written to. In this example, the file is written to AWSCLIV2.pkg in the current folder.\ncurl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; Run the standard macOS installer program, specifying the downloaded .pkg file as the source. Use the -pkg parameter to specify the name of the package to install, and the -target / parameter for which drive to install the package to. The files are installed to /usr/local/aws-cli, and a symlink is automatically created in /usr/local/bin. You must include sudo on the command to grant write permissions to those folders.\nsudo installer -pkg ./AWSCLIV2.pkg -target / To verify that the shell can find and run the aws command in your $PATH, use the following commands:\nwhich aws aws --version Setting up AWS CLI profiles Using AWS CLI profiles allows us to easily manage multiple sets of credentials, improving security and simplifying access to different AWS accounts and regions. This makes automating workflows and switching contexts more efficient and secure.\nIn this example we\u0026rsquo;re going to set up a profile called terraform-vpn, for lab purposes only.\nCreating the profile:\naws configure --profile terraform-vpn You will be prompted to enter your AWS credentials and configuration settings:\nAWS Access Key ID [None]: \u0026lt;your access key id\u0026gt; AWS Secret Access Key [None]: \u0026lt;your access key id\u0026gt; Default region name [None]: \u0026lt;your aws region\u0026gt; Default output format [None]: \u0026lt;I suggest using JSON as the output format\u0026gt; Verify that the configuration was successful:\naws sts get-caller-identity --profile terraform-vpn This command should return information about your AWS account, confirming that your credentials are working correctly.\nDiving into Automation Photo by NEOM on Unsplash\nTerraform configuration I\u0026rsquo;m considering you already have Terraform installed. If you don\u0026rsquo;t, the process is quite simple and there\u0026rsquo;s a ton of content on the Internet on how to do it.\nFirst, we\u0026rsquo;re going to create a main.cf file:\nterraform { required_version = \u0026#34;\u0026gt;= 0.12\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 3.0\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = var.aws_region profile = var.aws_profile } # Create a Virtual Private Gateway (VGW) resource \u0026#34;aws_vpn_gateway\u0026#34; \u0026#34;vgw\u0026#34; { vpc_id = var.vpc_id tags = { Name = var.vgw_name_tag } } # Create a Customer Gateway (CGW) resource \u0026#34;aws_customer_gateway\u0026#34; \u0026#34;cgw\u0026#34; { bgp_asn = var.bgp_asn ip_address = var.vpn_peer_ip type = \u0026#34;ipsec.1\u0026#34; tags = { Name = var.cgw_name_tag } } # Create a VPN Connection with BGP (dynamic routing) resource \u0026#34;aws_vpn_connection\u0026#34; \u0026#34;vpn_connection\u0026#34; { customer_gateway_id = aws_customer_gateway.cgw.id vpn_gateway_id = aws_vpn_gateway.vgw.id type = \u0026#34;ipsec.1\u0026#34; # BGP will be used so this must be set to false, or removed from the project # If you\u0026#39;re using static routes, set this to true static_routes_only = false tags = { Name = var.vpn_connection_name_tag } } Now that we have a main.tf, we need to create two more files:\nvariables.tf\nvariable \u0026#34;aws_profile\u0026#34; { description = \u0026#34;AWS CLI profile to use\u0026#34; type = string } variable \u0026#34;aws_region\u0026#34; { description = \u0026#34;AWS CLI profile to use\u0026#34; type = string } variable \u0026#34;vpc_id\u0026#34; { description = \u0026#34;AWS region to use\u0026#34; type = string } variable \u0026#34;vpn_peer_ip\u0026#34; { description = \u0026#34;Public IP address of the on-premises VPN peer\u0026#34; type = string } variable \u0026#34;bgp_asn\u0026#34; { description = \u0026#34;BGP Autonomous System Number for the Customer Gateway\u0026#34; type = number } variable \u0026#34;vgw_name_tag\u0026#34; { description = \u0026#34;Name tag for the Virtual Private Gateway\u0026#34; type = string } variable \u0026#34;cgw_name_tag\u0026#34; { description = \u0026#34;Name tag for the Customer Gateway\u0026#34; type = string } variable \u0026#34;vpn_connection_name_tag\u0026#34; { description = \u0026#34;Name tag for the VPN Connection\u0026#34; type = string } And, lastly, an outputs.tf file.\noutput \u0026#34;vpn_connection_id\u0026#34; { description = \u0026#34;VPN ID\u0026#34; value = aws_vpn_connection.vpn_connection.id } output \u0026#34;tunnel1_inside_cidr\u0026#34; { description = \u0026#34;Inside CIDR block for tunnel 1\u0026#34; value = aws_vpn_connection.vpn_connection.tunnel1_inside_cidr } output \u0026#34;tunnel1_aws_bgp_ip\u0026#34; { description = \u0026#34;AWS BGP peer IP for tunnel 1\u0026#34; value = aws_vpn_connection.vpn_connection.tunnel1_vgw_inside_address } output \u0026#34;tunnel1_customer_bgp_ip\u0026#34; { description = \u0026#34;Customer gateway BGP peer IP for tunnel 1\u0026#34; value = aws_vpn_connection.vpn_connection.tunnel1_cgw_inside_address } output \u0026#34;tunnel1_aws_public_ip\u0026#34; { description = \u0026#34;AWS public IP address for tunnel 1\u0026#34; value = aws_vpn_connection.vpn_connection.tunnel1_address } output \u0026#34;tunnel2_inside_cidr\u0026#34; { description = \u0026#34;Inside CIDR block for tunnel 2\u0026#34; value = aws_vpn_connection.vpn_connection.tunnel2_inside_cidr } output \u0026#34;tunnel2_aws_bgp_ip\u0026#34; { description = \u0026#34;AWS BGP peer IP for tunnel 2\u0026#34; value = aws_vpn_connection.vpn_connection.tunnel2_vgw_inside_address } output \u0026#34;tunnel2_customer_bgp_ip\u0026#34; { description = \u0026#34;Customer gateway BGP peer IP for tunnel 2\u0026#34; value = aws_vpn_connection.vpn_connection.tunnel2_cgw_inside_address } output \u0026#34;tunnel2_aws_public_ip\u0026#34; { description = \u0026#34;AWS public IP address for tunnel 2\u0026#34; value = aws_vpn_connection.vpn_connection.tunnel2_address } output \u0026#34;tunnel1_preshared_key\u0026#34; { description = \u0026#34;Preshared key for tunnel 1\u0026#34; value = aws_vpn_connection.vpn_connection.tunnel1_preshared_key sensitive = true } output \u0026#34;tunnel2_preshared_key\u0026#34; { description = \u0026#34;Preshared key for tunnel 2\u0026#34; value = aws_vpn_connection.vpn_connection.tunnel2_preshared_key sensitive = true } At this point, your Terraform configuration should have the following structure:\n‚îú‚îÄ‚îÄ main.tf ‚îú‚îÄ‚îÄ outputs.tf ‚îî‚îÄ‚îÄ variables.tf Initializing the Terraform Configuration To initialize Terraform and download required providers:\nterraform init Planning the changes To preview the changes that Terraform will apply, we will use the terraform plan command, setting all the variables.\nterraform plan \\ -var=\u0026#34;aws_profile=terraform-vpn\u0026#34; \\ -var=\u0026#34;aws_region=your_aws_region\u0026#34; \\ -var=\u0026#34;vpc_id=your_vpc_id\u0026#34; \\ -var=\u0026#34;vpn_peer_ip=your_on_prem_peer_public_ip\u0026#34; \\ -var=\u0026#34;bgp_asn=your_on_prem_bgp_asn\u0026#34; \\ -var=\u0026#34;vgw_name_tag=my-vpn-gateway\u0026#34; \\ -var=\u0026#34;cgw_name_tag=my-customer-gateway\u0026#34; \\ -var=\u0026#34;vpn_connection_name_tag=my-vpn-connection\u0026#34; \\ -out=vpn_to_onprem This command will generate a detailed execution plan, and the resulting changes will be saved in a file named vpn_to_onprem. Feel free to modify this file name as needed, depending on your preferences or environment.\nApplying the changes Apply the planned changes to provision the infrastructure based on the output file generated by terraform plan:\nterraform apply \u0026#34;vpn_to_onprem\u0026#34; Once the command finishes running, Terraform will display some of the outputs defined in the outputs.tf file. However, you\u0026rsquo;ll notice that sensitive information, such as pre-shared keys (PSKs), is not shown directly in the output for security reasons.\nViewing Terraform Outputs As mentioned above, the outputs defined in the outputs.tf file are automatically provided by Terraform and stored in the terraform.tfstate file. To view all outputs, including sensitive data, you have two options:\nOption 1: Use the JSON flag You can retrieve all outputs in JSON format, which will include sensitive information without redaction:\nterraform output -json Option 2: Manually Inspect the State File Open the terraform.tfstate file and manually search for the sensitive data. This file contains the full configuration, including sensitive details like PSKs and IP addresses. Note that handling this file requires caution as it contains critical information.\nTerraform Destroy One of Terraform\u0026rsquo;s key benefits is how easily changes can be reverted with the terraform destroy command.\nIf you need to revert the changes we previously applied, use the following syntax:\nterraform destroy \\ -var=\u0026#34;aws_profile=terraform-vpn\u0026#34; \\ -var=\u0026#34;aws_region=your_aws_region\u0026#34; \\ -var=\u0026#34;vpc_id=your_vpc_id\u0026#34; \\ -var=\u0026#34;vpn_peer_ip=your_on_prem_peer_public_ip\u0026#34; \\ -var=\u0026#34;bgp_asn=your_on_prem_bgp_asn\u0026#34; \\ -var=\u0026#34;vgw_name_tag=my-vpn-gateway\u0026#34; \\ -var=\u0026#34;cgw_name_tag=my-customer-gateway\u0026#34; \\ -var=\u0026#34;vpn_connection_name_tag=my-vpn-connection\u0026#34; \\ Configuring our VPN peer (Cisco Router) To wrap up, below is a Python script I created. This script generates the VPN and BGP configuration for a Cisco router based on Terraform outputs. Please use with caution and always double-check before applying it in a production environment.\nKey points:\nAWS uses by default BGP ASN 64512 On my lab environment I\u0026rsquo;m using BGP ASN 65000 on the VPN peer Both of the settings mentioned above are hardcoded on the script. Change as needed.\nimport subprocess import json def get_terraform_outputs(): try: # Get the Terraform output in JSON format result = subprocess.run( [\u0026#34;terraform\u0026#34;, \u0026#34;output\u0026#34;, \u0026#34;-json\u0026#34;], capture_output=True, text=True, check=True ) return json.loads(result.stdout) except subprocess.CalledProcessError as e: print(f\u0026#34;Error: {e}\u0026#34;) return {} def generate_cisco_config(outputs): # Sets variables based on Terraform outputs tunnel1_aws_bgp_ip = outputs.get(\u0026#34;tunnel1_aws_bgp_ip\u0026#34;, {}).get(\u0026#34;value\u0026#34;, \u0026#34;\u0026#34;) tunnel2_aws_bgp_ip = outputs.get(\u0026#34;tunnel2_aws_bgp_ip\u0026#34;, {}).get(\u0026#34;value\u0026#34;, \u0026#34;\u0026#34;) tunnel1_customer_bgp_ip = outputs.get(\u0026#34;tunnel1_customer_bgp_ip\u0026#34;, {}).get(\u0026#34;value\u0026#34;, \u0026#34;\u0026#34;) tunnel2_customer_bgp_ip = outputs.get(\u0026#34;tunnel2_customer_bgp_ip\u0026#34;, {}).get(\u0026#34;value\u0026#34;, \u0026#34;\u0026#34;) tunnel1_aws_public_ip = outputs.get(\u0026#34;tunnel1_aws_public_ip\u0026#34;, {}).get(\u0026#34;value\u0026#34;, \u0026#34;\u0026#34;) tunnel2_aws_public_ip = outputs.get(\u0026#34;tunnel2_aws_public_ip\u0026#34;, {}).get(\u0026#34;value\u0026#34;, \u0026#34;\u0026#34;) tunnel1_preshared_key = outputs.get(\u0026#34;tunnel1_preshared_key\u0026#34;, {}).get(\u0026#34;value\u0026#34;, \u0026#34;\u0026#34;) tunnel2_preshared_key = outputs.get(\u0026#34;tunnel2_preshared_key\u0026#34;, {}).get(\u0026#34;value\u0026#34;, \u0026#34;\u0026#34;) # Cisco config block config = f\u0026#39;\u0026#39;\u0026#39; !!! DOUBLE-CHECK BEFORE APPLYING IN PRODUCTION !!! crypto isakmp policy 1 encryption aes 128 authentication pre-share group 2 lifetime 28800 hash sha crypto keyring aws_vpn_keyring1 pre-shared-key address {tunnel1_aws_public_ip} key {tunnel1_preshared_key} crypto keyring aws_vpn_keyring2 pre-shared-key address {tunnel2_aws_public_ip} key {tunnel2_preshared_key} crypto isakmp profile aws_vpn_isakmp_profile1 match identity address {tunnel1_aws_public_ip} keyring aws_vpn_keyring1 crypto isakmp profile aws_vpn_isakmp_profile2 match identity address {tunnel2_aws_public_ip} keyring aws_vpn_keyring2 crypto ipsec transform-set aws_vpn_transform_set esp-aes 128 esp-sha-hmac mode tunnel crypto ipsec profile aws_vpn_ipsec_profile set pfs group2 set security-association lifetime seconds 3600 set transform-set aws_vpn_transform_set interface Tunnel1 ip address {tunnel1_customer_bgp_ip} 255.255.255.252 ip tcp adjust-mss 1360 ip virtual-reassembly tunnel source \u0026lt;your tunnel source ip/interface\u0026gt; tunnel destination {tunnel1_aws_public_ip} tunnel mode ipsec ipv4 tunnel protection ipsec profile aws_vpn_ipsec_profile no shutdown exit interface Tunnel2 ip address {tunnel2_customer_bgp_ip} 255.255.255.252 ip tcp adjust-mss 1360 ip virtual-reassembly tunnel source \u0026lt;your tunnel source ip/interface\u0026gt; tunnel destination {tunnel2_aws_public_ip} tunnel mode ipsec ipv4 tunnel protection ipsec profile aws_vpn_ipsec_profile no shutdown exit router bgp 65000 neighbor {tunnel1_aws_bgp_ip} remote-as 64512 neighbor {tunnel1_aws_bgp_ip} activate neighbor {tunnel1_aws_bgp_ip} timers 10 30 30 neighbor {tunnel2_aws_bgp_ip} remote-as 64512 neighbor {tunnel2_aws_bgp_ip} activate neighbor {tunnel2_aws_bgp_ip} timers 10 30 30 address-family ipv4 unicast neighbor {tunnel1_aws_bgp_ip} remote-as 64512 neighbor {tunnel1_aws_bgp_ip} timers 10 30 30 neighbor {tunnel1_aws_bgp_ip} activate neighbor {tunnel1_aws_bgp_ip} soft-reconfiguration inbound neighbor {tunnel2_aws_bgp_ip} remote-as 64512 neighbor {tunnel2_aws_bgp_ip} timers 10 30 30 neighbor {tunnel2_aws_bgp_ip} activate neighbor {tunnel2_aws_bgp_ip} soft-reconfiguration inbound end !!! DOUBLE-CHECK BEFORE APPLYING IN PRODUCTION !!! \u0026#39;\u0026#39;\u0026#39; return (config) if __name__ == \u0026#34;__main__\u0026#34;: outputs = get_terraform_outputs() if outputs: cisco_config = generate_cisco_config(outputs) print(cisco_config) ","permalink":"http://localhost:1313/posts/terraform-aws-vpn/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eIn this post, I\u0026rsquo;ll walk you through how to use Terraform to set up a VPN site-to-site connection on AWS, leveraging Infrastructure as Code (IaC) to make the process quicker, easier, and fully repeatable.\u003c/p\u003e\n\u003cp\u003eFor the examples below, we‚Äôll build a cloud infrastructure using a Virtual Private Gateway (VGW). In a future post, we‚Äôll explore using a Transit Gateway (TGW) for more complex setups.\u003c/p\u003e\n\u003cp\u003eI‚Äôm running everything on a MacBook, but you can easily replicate these steps on a Linux jump host or any similar environment.\u003c/p\u003e","title":"Automating AWS site-to-site VPNs with Terraform"},{"content":"In this post, I‚Äôll walk you through a few simple steps to resolve SSH Key Exchange and ensure smooth SSH connections.\nWhile working on a network automation task, I encountered an error while trying to establish connections to some legacy devices: Unable to negotiate with x.x.x.x port 22: no matching key exchange method found. Their offer: diffie-hellman-group-exchange-sha1, diffie-hellman-group14-sha1.\nThe error message indicates that the only available key exchange methods are diffie-hellman-group-exchange-sha1 and diffie-hellman-group14-sha1. To address this, simply update your /etc/ssh/ssh_config file by adding the following lines at the end:\nHostKeyAlgorithms +ssh-rsa,ssh-dss KexAlgorithms +diffie-hellman-group1-sha1,diffie-hellman-group14-sha1 Make sure to enable the required key exchange methods for your use case and save the ssh_config file before attemping a new connection.\nBy following these steps, you should be able to resolve the key exchange issues and maintain stable SSH connections with legacy devices. However, keep in mind that you\u0026rsquo;re enabling deprecated and weaker cryptographic algorithms.\n","permalink":"http://localhost:1313/posts/resolving-ssh-key-exchange-errors/","summary":"\u003cp\u003eIn this post, I‚Äôll walk you through a few simple steps to resolve SSH Key Exchange and ensure smooth SSH connections.\u003c/p\u003e\n\u003cp\u003eWhile working on a network automation task, I encountered an error while trying to establish connections to some legacy devices:\n\u003ccode\u003eUnable to negotiate with x.x.x.x port 22: no matching key exchange method found. Their offer: diffie-hellman-group-exchange-sha1, diffie-hellman-group14-sha1.\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eThe error message indicates that the only available key exchange methods are \u003cstrong\u003ediffie-hellman-group-exchange-sha1\u003c/strong\u003e and \u003cstrong\u003ediffie-hellman-group14-sha1\u003c/strong\u003e. To address this, simply update your \u003ccode\u003e/etc/ssh/ssh_config\u003c/code\u003e file by adding the following lines at the end:\u003c/p\u003e","title":"Resolving SSH Key Exchange Errors"},{"content":"I\u0026rsquo;m now a member of the Cisco Insider Champion group!\nReally glad to become part of this team of amazing IT professionals!\nMy badge URL: https://www.credly.com/badges/f66fb7c6-b852-47c3-b9d2-42e8554e58c7/public_url\n","permalink":"http://localhost:1313/posts/cisco-insider-champion/","summary":"\u003cp\u003eI\u0026rsquo;m now a member of the \u003cstrong\u003eCisco Insider Champion\u003c/strong\u003e group!\u003c/p\u003e\n\u003cp\u003eReally glad to become part of this team of amazing IT professionals!\u003c/p\u003e\n\u003cp\u003eMy badge URL: \u003ca href=\"https://www.credly.com/badges/f66fb7c6-b852-47c3-b9d2-42e8554e58c7/public_url\"\u003ehttps://www.credly.com/badges/f66fb7c6-b852-47c3-b9d2-42e8554e58c7/public_url\u003c/a\u003e\u003c/p\u003e","title":"Cisco Insider Champion 2024"},{"content":"My journey I officially started my career in IT in 2006, but my first steps with networks occurred a few years before that, between 2002 and 2003.\nIn 2006, I began as an intern in user support at a government agency. I started by formatting computers and quickly progressed to a point where I dealt with the entire network infrastructure including switches, firewalls, proxy, file servers, AD, etc.\nAfter completing this internship, I went to another company, again as an intern, where I spent another 2 years before being officially hired. In total, I worked there for almost 10 years and had the opportunity to learn from exceptional professionals. While working there, I obtained the CCNA certification in 2008, then the CCDA certification in 2013 and completed the CCNP Routing \u0026amp; Switching certification track in 2017.\nRight after finishing the last exam of the track and obtaining the CCNP R\u0026amp;S certification, I bought some books and started studying the content from INE and the now-defunct IPExpert. When I started studying, the exam was still the CCIE Routing \u0026amp; Switching v5.0. At some point, the exam was updated to version CCIE Routing \u0026amp; Switching v5.1 and shortly after that, I changed jobs. It was February 2018 and I started working as a Technical Leader of a team composed of network and security engineers. Due to the high workload, unfortunately, I had to reduce the pace of my studies. I didn\u0026rsquo;t completely stop studying, but I was already studying at a very slow pace.\nStill at the same company, at the end of 2019, I joined a newly created team that we called the Solutions Architecture team. I became responsible not only for the implementation but also for the Design of all the company\u0026rsquo;s network projects. Also in 2019, I needed to renew my certifications and that\u0026rsquo;s how I obtained the Cisco CCDP certification, which at the time was still a separate certification and later became a specialization, currently called Cisco Certified Specialist - Enterprise Design.\nIn 2020, with the start of the pandemic and remote work, I was able to reorganize my routine in order to fully focus on studying for the CCIE. The exam was already in its third version since I had started studying, no longer called CCIE Routing \u0026amp; Switching but CCIE Enterprise Infrastructure v1.0.\nSo, in summary, I officially started studying for the CCIE in 2017, after completing the CCNP, at a time when the exam was still the CCIE R\u0026amp;S v5.0. Due to routine issues, I slowed down my study pace and only managed to resume in 2020. Basically, from the beginning of 2020 until December 2022 (the date I took the lab), my focus was exclusively on studying. In the next post, I will go into more detail about strategies, study hours, among others.\nAll the information regarding the exam that you will find here is public and is available on Cisco\u0026rsquo;s website.\nUnderstanding the CCIE exam Eligibility to the CCIE LAB The first point to understand is that in order to schedule the CCIE LAB, you need to have passed the 350-401 ENCOR exam within a period of 3 years. If you are eligible, simply schedule the lab through the scheduling tool: https://ccie.cloudapps.cisco.com/CCIE/Schedule_Lab/CCIEOnline/CCIEOnline\nMany people don\u0026rsquo;t know but unlike exams like CCNA and CCNP, which are conducted at accredited training centers, the CCIE LAB is conducted in person at specific Cisco\u0026rsquo;s testing facilities listed below:\nSource: https://www.cisco.com/c/en/us/training-events/training-certifications/certifications/expert/ccie-lab-exam-locations.html\nNote in the table above that there are the so-called Mobile Labs. As the name suggests, these are temporary labs that occur in certain locations. At the time of writing this post (January 2024), I noticed that there are some dates when the lab will be in Brazil in April 2024. However, when checking through the scheduling tool, I did not find available slots. The downside of Mobile Labs is that slots tend to fill up quickly. You can check the dates of the Mobile Labs here: https://learningnetwork.cisco.com/s/mobile-lab-scheduler\nFor logistical reasons, I chose to take the lab at a fixed location and scheduled my exam at the infamous Cisco\u0026rsquo;s Building 5 in Richardson, Texas.\nCisco Building 5 - Richardson, Texas - December 2022\nBy doing it this way, I didn\u0026rsquo;t have any difficulty in finding availability for the LAB schedule. What needs to be considered here are the travel costs. Weigh all the items and decide what\u0026rsquo;s most suitable for your case.\nContinuing, the CCIE LAB lasts for 8 hours and is divided into 2 modules:\nCCIE Lab Modules\nTo pass, you need to achieve a minimum score in each of the modules as well as an overall minimum score (pass score). It\u0026rsquo;s not enough to get the maximum score in one module and fall below the minimum in the other: in this condition, you won\u0026rsquo;t pass.\nThe minimum score values are not disclosed. The important thing is to understand that this is the scoring format of the exam:\nCCIE Score Evaluation\nAnother critical detail is that there is no partial scoring. For example: let\u0026rsquo;s say in a certain task you configured everything correctly but used a different ACL name than what the question asked for. In this case, you won\u0026rsquo;t get any points for that question. This is one of the factors that makes the exam so difficult: attention to details is essential.\nModule 1: Design (3 hours) The objective of this module is to measure the ability to create, analyze, validate, and optimize network designs, which is the foundation for all deployment activities. Candidates will need to:\nUnderstand the capabilities of different technologies, solutions, and services. Translate customer requirements into solutions. Assess the readiness of the infrastructure to support the proposed solutions. The module is scenario-based, with no access to any devices. Candidates will receive a set of documentation to review before answering the questions.\nExamples of documentation include email threads, high-level designs, network topology diagrams, customer requirements and constraints, etc. Examples of questions include drag and drop, single-choice multiple-answer, multiple-choice, etc. During this module, backward navigation will be disabled. Therefore, candidates will not have full visibility of all questions in this module. The point values associated with each item are not displayed in this module.\nModule 2: Deploy, Operate and Optimize (5 hours) In this module, candidates will deploy, operate, and optimize network technologies and solutions.\nDeploy Candidates will build the network according to project specifications, customer requirements, and constraints. All steps necessary for a successful network implementation will be covered, including configuration, integration, and troubleshooting in the commissioning of technologies and solutions, as per the exam topics.\nOperate and Optimize Candidates will operate and optimize network technologies and solutions. This includes monitoring network health and performance, configuring the network to improve service quality, reduce outages, mitigate disruptions, lower operational costs, and maintain high availability, reliability, and security, as well as diagnosing potential issues and adjusting configurations to align with business objectives and/or technical requirements. This module provides a setup very close to a real production network environment and will consist of practical items (device access) as well as web-based items. Whenever possible, a virtualized lab environment will be used in this module.\nDuring this module, backward navigation will be enabled. Candidates will have full visibility of all questions in this module. Point values associated with each item are displayed in this module.\nSource: https://learningnetwork.cisco.com/s/article/ccie-practical-exam-format\nStudy Tips First Steps Start by downloading the CCIE Enterprise Infrastructure Learning Matrix spreadsheet from the link below. Created and maintained by Cisco, this spreadsheet lists all exam topics with recommended study materials.\nCisco CCIE Enterprise Learning Matrix\nThe first column in the spreadsheet, labeled \u0026ldquo;Level\u0026rdquo;, is for rating your knowledge from 1 to 5 on each topic. Although it comes pre-filled, adjust it honestly to reflect your actual knowledge. This way, you\u0026rsquo;ll know where to focus your studies. Update it regularly as you progress.\nGeneral Tips Master English\nIf English isn\u0026rsquo;t your native language, prioritize learning it. Understanding all details and avoiding exam traps is critical.\nAvoid Mindless Study\nStudying hours without strategy or focus is counterproductive. Develop a plan and use the Learning Matrix to find weak spots.\nStart Small\nBegin with a few hours of study each day, gradually increasing your time. Burnout is a real risk if you push too hard at the start.\nFocus on Specific Topics\nAt the beginning, create simple topologies. Avoid working with large topologies that use multiple protocols until you have a solid understanding of each individually.\nSelective Reading\nDon‚Äôt try to read books from cover to cover. Instead, focus on individual technologies and protocols.\nEmbrace Repetition\nRe-reading topics is normal. Accept that learning requires revisiting concepts multiple times.\nHands-on Configuration\nRefer back to theory while configuring networks. This approach helps solidify learning.\nAnalyze Packet Captures\nLearn how commands affect protocols by analyzing packet captures. Examine layers in depth and understand how configurations alter specific bits or attributes.\nStudy Design\nExplore different design methods. There are many ways to reach the same result‚Äîunderstand the pros and cons of each to make informed decisions.\nFinal Study Phase\nApproximately three months before the lab exam, aim for at least 8 hours of daily study, focusing on configuring and troubleshooting complex topologies. Train yourself to maintain focus during long, stressful exam conditions.\nRecommended Books Here are some books that I personally used and consider highly relevant:\nCCIE Routing \u0026amp; Switching v5.0, Volume 1, 5th Edition [Kocharians \u0026amp; Paluch] CCIE Routing \u0026amp; Switching v5.0, Volume 2, 5th Edition [Kocharians \u0026amp; Paluch] Definitive MPLS Network Designs [Guichard] IP Multicast, Volume I [Loveless, Blair, Durai] IPv6 Fundamentals, 2nd Edition [Graziani] MPLS Fundamentals [De Ghein] Optimal Routing Design [White, Slice, Rentana] Routing TCP/IP, Volume 1, 2nd Edition [Doyle \u0026amp; Carrol] Routing TCP/IP, Volume 2, 2nd Edition [Doyle] Troubleshooting BGP [Edgeworth, Jain] Routing \u0026amp; Switching Traditional protocols still make up a significant portion of the exam. I recommend INE‚Äôs older materials for Routing \u0026amp; Switching (CCIE R\u0026amp;S v5.0), including both videos and workbooks. They provide unbeatable content. I also used EVE-NG and Cisco CML for labs. Make sure to study using the software versions that will be used in the exam. You can find this information in the CCIE Enterprise Infrastructure (v1.1) Equipment and Software List on Cisco\u0026rsquo;s website: Cisco CCIE Exam Format and Equipment SD-WAN Start with Cisco\u0026rsquo;s Configuration Guides and Design Guides. Cisco Live presentations are another valuable resource (available on ciscolive.com). For lab practice, if you have the hardware, you can use VMware to virtualize SD-WAN components. If not, consider renting lab sessions from Cisco\u0026rsquo;s CCIE Enterprise Infrastructure Practice Labs: Cisco CCIE Practice Labs DNA Center / SD-Access / ISE If you lack access to these in production environments, you‚Äôll need lab sessions using Cisco\u0026rsquo;s CCIE Practice Labs. These can be scheduled in advance and usually last four hours per session. It‚Äôs a worthwhile investment. Automation Python: Use a Linux machine (physical or virtual) to connect to routers running IOS XE. Start with basic Python skills if you‚Äôre new to the language. EEM Applets and Guestshell: Practice on virtual routers like CSR 1000v or Catalyst 8000v. APIs (vManage and DNA Center): Utilize Cisco DevNet Sandbox for free access, but for full lab control, schedule sessions in the CCIE Practice Labs. ","permalink":"http://localhost:1313/posts/journey-to-ccie/","summary":"\u003ch2 id=\"my-journey\"\u003eMy journey\u003c/h2\u003e\n\u003cp\u003eI officially started my career in IT in 2006, but my first steps with networks occurred a few years before that, between 2002 and 2003.\u003c/p\u003e\n\u003cp\u003eIn 2006, I began as an intern in user support at a government agency. I started by formatting computers and quickly progressed to a point where I dealt with the entire network infrastructure including switches, firewalls, proxy, file servers, AD, etc.\u003c/p\u003e\n\u003cp\u003eAfter completing this internship, I went to another company, again as an intern, where I spent another 2 years before being officially hired. In total, I worked there for almost 10 years and had the opportunity to learn from exceptional professionals. While working there, I obtained the \u003cstrong\u003eCCNA\u003c/strong\u003e certification in \u003cstrong\u003e2008\u003c/strong\u003e, then the \u003cstrong\u003eCCDA\u003c/strong\u003e certification in \u003cstrong\u003e2013\u003c/strong\u003e and completed the \u003cstrong\u003eCCNP Routing \u0026amp; Switching\u003c/strong\u003e certification track in \u003cstrong\u003e2017\u003c/strong\u003e.\u003c/p\u003e","title":"My Journey to CCIE"},{"content":"Introduction This post is a summary of OSPF that I compiled during my CCIE journey, gathering information from RFCs, books, Cisco documentation, blogs, and other sources.\nIMPORTANT: I‚Äôm still in the process of converting this page from my original notes, so some information may be missing, and the formatting may not yet be fully refined.\nOSPFv2 OSPFv2 is documented under RFC 2328, which states the following:\nOSPF routes IP packets based solely on the destination IP address found in the IP packet header. IP packets are routed \u0026ldquo;as is\u0026rdquo; \u0026ndash; they are not encapsulated in any further protocol headers as they transit the Autonomous System. OSPF is a dynamic routing protocol. It quickly detects topological changes in the AS (such as router interface failures) and calculates new loop-free routes after a period of convergence.\nThis period of convergence is short and involves a minimum of routing traffic.\nIn a link-state routing protocol, each router maintains a database describing the Autonomous System\u0026rsquo;s topology. This database is referred to as the link-state database. Each participating router has an identical database. Each individual piece of this database is a particular router\u0026rsquo;s local state (e.g., the router\u0026rsquo;s usable interfaces and reachable neighbors).\nThe router distributes its local state throughout the Autonomous System by flooding.\nOSPFv2 Packet Header OSPFv3 OSPFv3 is documented under RFC 5340\nAbstract:\nThis document describes the modifications to OSPF to support version 6 of the Internet Protocol (IPv6). The fundamental mechanisms of OSPF (flooding, Designated Router (DR) election, area support, Short Path First (SPF) calculations, etc.) remain unchanged. However, some changes have been necessary, either due to changes in protocol semantics between IPv4 and IPv6, or simply to handle the increased address size of IPv6. These modifications will necessitate incrementing the protocol version from version 2 to version 3. OSPF for IPv6 is also referred to as OSPF version 3 (OSPFv3).\nChanges between OSPF for IPv4, OSPF Version 2, and OSPF for IPv6 as described herein include the following. Addressing semantics have been removed from OSPF packets and the basic Link State Advertisements (LSAs). New LSAs have been created to carry IPv6 addresses and prefixes. OSPF now runs on a per-link basis rather than on a per-IP-subnet basis. Flooding scope for LSAs has been generalized. Authentication has been removed from the OSPF protocol and instead relies on IPv6\u0026rsquo;s Authentication Header and Encapsulating Security Payload (ESP).\nEven with larger IPv6 addresses, most packets in OSPF for IPv6 are almost as compact as those in OSPF for IPv4. Most fields and packet- size limitations present in OSPF for IPv4 have been relaxed. In addition, option handling has been made more flexible.\nAll of OSPF for IPv4\u0026rsquo;s optional capabilities, including demand circuit support and Not-So-Stubby Areas (NSSAs), are also supported in OSPF for IPv6.\nOSPFv3 Packet Header Multicast Groups OSPFv2 Multicast groups Multicast Group Description 224.0.0.5 All OSPF Routers 224.0.0.6 All DR Routers OSPFv3 Multicast groups Multicast Group Description FF02::5 OSPFv3 Routers FF02::6 OSPFv3 Designated Routers Timers Interval Broadcast Non-Broadcast Point-to-Point Point-to-Multipoint Point-to-Multipoint Non-Broadcast Hello 10 30 10 30 30 Dead 40 120 40 90 120 Wait 40 90 Retransmit 5 5 5 5 5 OSPFv2 LSA Types LSA Type 1 ‚Äì Router LSA Route Type: Intra-Area CLI Command:\nshow ip ospf database router Description: Each router within an area floods a Type 1 Router LSA to other routers in the same area. This LSA describes all of the router‚Äôs interfaces, neighbor relationships, and metrics. LSA Type 2 ‚Äì Network LSA Route Type: Intra-Area CLI Command:\nshow ip ospf database network Description: Generated by the Designated Router (DR) on multi-access networks (e.g., Ethernet). Represents the multi-access network and all attached routers as a pseudonode (virtual router). Lists all routers connected to the network segment, including the DR itself. Like Type 1 LSAs, Type 2 LSAs are flooded only within the originating area. LSA Type 3 ‚Äì Network Summary LSA Route Type: Inter-Area CLI Command:\nshow ip ospf database summary Description: Originated by ABRs. Advertises destinations outside the local area into the area. Used by ABRs to inform internal routers about reachable destinations in other areas. ABRs also advertise routes from their own areas into the backbone (Area 0) using Type 3 LSAs. Can also be used to advertise default routes internal to the OSPF autonomous system. LSA Type 4 ‚Äì ASBR Summary LSA Route Type: Inter-Area CLI Commands:\nshow ip ospf database asbr-summary\nshow ip ospf border-routers Description: Also originated by ABRs. Similar to Type 3 LSAs but specifically advertises routes to ASBRs. The destination is always a host address representing the ASBR router itself. LSA Type 5 ‚Äì External LSA Route Type: External Routes (E1/E2) CLI Commands:\nshow ip ospf database external\nshow ip ospf border-routers Description: Originated by ASBRs. Advertises destinations external to the OSPF autonomous system (e.g., routes from BGP, static). Can also advertise default routes from outside the OSPF domain. Flooded throughout the entire OSPF domain. LSA Type 7 ‚Äì NSSA External LSA Route Type: External Routes (N1/N2) CLI Commands:\nshow ip ospf database external\nshow ip ospf border-routers Description: Originated by ASBRs within Not-So-Stubby Areas (NSSA). Similar to Type 5 LSAs but scoped only within the originating NSSA. Not flooded beyond the NSSA; may be translated to Type 5 by the ABR for propagation beyond the NSSA. OSPF Options Down Bit The down bit helps prevent routing loops between MP-BGP and OSPF when LSA Type 3 are used, but not when External Routes are announced. With LSA Type 5 and Type 7, there is a new field checked to avoid loops is called the Tag field. When a PE redistributes a route from MP-BGP into OSPF as LSA Type 5 or LSA Type 7, it adds a Tag to the route (tag 3989725929 by default). So if another PE receives an LSA Type 5 or LSA Type 7 with this tag, it doesn‚Äôt redistribute it back into MP-BGP. The default Tag value of 3989725929 can be changed to any other value (redistribute bgp 65001 subnets tag 100). N Bit This bit is used in hello packets for OSPF NSSA routers. When the N-bit is not supported, the routers won‚Äôt become neighbors. P Bit The P-bit (P stands for propagate) is set in the Options field of an LSA type 7. P-bit tells the ABR if the LSA type 7 should be translated into a LSA type 5 or not. P-bit is automatically set by default for all prefixes that are redistributed by the ASBR and are flooded within that area only. Can be disabled with nssa-only keyword. The P-bit is not set only when the NSSA ASBR and NSSA ABR are the same router for the area. This P-bit is automatically set by the NSSA ABR (also the Forwarding Address (FA) is copied from Type 7 LSA). Note:\nIf a router is attached to another AS and is also an NSSA ABR, it may originate a both a Type 5 and a Type 7 LSA for the same network. The Type 5 LSA will be flooded to the backbone and the Type 7 will be flooded into the NSSA. If this is the case, the P-bit must be reset (P=0) in the Type 7 LSA so the Type 7 LSA isn‚Äôt again translated into a Type 5 LSA by another NSSA ABR. Routing Bit The routing bit is not a part of the LSA itself. Routing Bit is an internal maintenance bit used by IOS indicating that the route to the destination advertised by this LSA is valid. So when you see \u0026ldquo;Routing Bit Set on this LSA,\u0026rdquo; it means that the route to this destination is in the routing table. Demand Circuit Per RFC 1793, Extending OSPF to Support Demand Circuits, ‚ÄúOSPF Hellos and the refresh of OSPF routing information are suppressed on demand circuits, allowing the underlying data-link connections to be closed when not carrying application traffic.‚Äù This feature allows low-speed and pay-per-use links, such as analog dial and ISDN, to run OSPF without the need for periodic hellos and LSA flooding. Periodic hellos are only suppressed for point-to-point and point-to-multipoint OSPF network types. This feature is enabled with the interface-level command ip ospf demand-circuit and is negotiated as part of the neighbor adjacency establishment, thus only one OSPF router on the segment requires that the feature be enabled. If routers on the segment do not support it, it will just ignore the option in the HELLO packet, but OSPF neighbors will still be established. Flood Reduction Per RFC 2328, OSPF Version 2, ‚Äúan LSA\u0026rsquo;s LS age is never incremented past the value MaxAge.\u0026quot;, so when the Link State Age reaches MaxAge, \u0026ldquo;the router must attempt to flush the LSA\u0026hellip; by reflooding the MaxAge LSA just as if it was a newly originated LSA\u0026rdquo;. In Cisco‚Äôs IOS implementation of OSPF, the MaxAge value is 3600 seconds, or 60 minutes. To ensure that an LSA is not aged out, which means it will be flushed from the OSPF database, each LSA is reflooded after 30 minutes, regardless of whether the topology is stable or not. This periodic flooding behavior is commonly referred to as the ‚Äúparanoid update.‚Äù The ip ospf flood-reduction feature stops unnecessary LSA flooding by setting the DoNotAge (DNA) bit in the LSA, removing the requirement for the periodic refresh. This needs to be enabled on links with OSPF neighbors attached, as on the other links, as there are no neighbors, no LSAs are sent anyways. Stub Router Feature Prevents a router from becoming a transit router (nontransit router). Nontransit routers only forward packets to and from locally attached subnets. max-metric router-lsa\nonly transit networks max-metric router-lsa include-stub transit networks + stub networks (ex: loopback) max-metric router-lsa on-startup announce-time max-metric router-lsa on-startup wait-for-bgp OSPF Graceful Shutdown Similar to using the shutdown command on an interface, there is also a shutdown command for OSPF. This allows you to gracefully stop the OSPF routing process without removing the configuration from your router. You can do this globally or on the interface level.\nIf you want to do a graceful shutdown globally, you have to use the shutdown command under the OSPF process. This will:\nDrop all neighbor adjacencies Flush all LSAs that the router originated by setting the age to 3600 seconds Send hello packets with the DR/BDR set to 0.0.0.0 and an empty neighbor list. This will trigger other OSPF routers to fall back to init state. Stop sending/receiving OSPF packets If you shut OSPF on the interface level with the ip ospf shutdown command, it will do this:\nDrop all neighbor adjacencies on the interface you selected Flood updated LSAs where all information (prefix, neighbors) from the selected interface is removed Send hello packets with the DR/BDR set to 0.0.0.0 and an empty neighbor list. This will trigger other OSPF router to fall back to init state Stop sending/receiving OSPF packets OSPF Filtering OSPF RIB Filtering distribute-list with ACL distribute-list with route-map Administrative Distance RIB filtering does not stop the flooding of LSAs within the area Inter Area Routes area X range 10.0.0.0 not-advertise Only effective when ABR is translating from Type 1 LSA to a Type 3 LSA (learn from INTRA Area and advertise INTER Area) area X filter-list prefix XXX in prevent routes from being advertised TO the specified area area X filter-list prefix XXX out prevent routes from being advertised FROM the specified area External Routes (LSA Type 5/7) Effective to filter when going between areas Has to be applied on the device that is doing the translation into Type 5 LSA\nsummary-address 10.0.0.0 255.255.255.0 not-advertise summary-address 10.0.0.0 255.255.255.0 nssa-only Inter-Area OSPF is Distance Vector (OSPF Loop Prevention) The content below is from an amazing post by Jeff Doyle, available on the link below, where Jeff gets into details about OSPF\u0026rsquo;s behavior with Intra-Area deployments.\nSource: https://www.networkworld.com/article/2348778/my-favorite-interview-question.html\nWhy does OSPF require all traffic between non-backbone areas to pass through a backbone area (area 0)?\nComparing three fundamental concepts of link state protocols, concepts that even most OSPF beginners understand, easily derives the answer to the above question.\nThe first concept:\nEvery link state router floods information about itself, its links, and its neighbors to every other router. From this flooded information each router builds an identical link state database. Each router then independently runs a shortest-path-first calculation on its database ‚Äì a local calculation using distributed information ‚Äì to derive a shortest-path tree. This tree is a sort of map of the shortest path to every other router.\nOne of the advantages of link state protocols is that the link state database provides a ‚Äúview‚Äù of the entire network, preventing most routing loops. This is in contrast to distance vector protocols, in which route information is passed hop-by-hop through the network and a calculation is performed at each hop ‚Äì a distributed calculation using local information. Each router along a route is dependent on the router before it to perform its calculations correctly and then correctly pass along the results. When a router advertises the prefixes it learns to its neighbors it‚Äôs basically saying, ‚ÄúI know how to reach these destinations.‚Äù And because each distance vector router knows only what its neighbors tell it, and has no ‚Äúview‚Äù of the network beyond the neighbors, the protocol is vulnerable to loops.\nThe second concept:\nWhen link state domains grow large, the flooding and the resulting size of the link state database becomes a scaling problem. The problem is remedied by breaking the routing domain into areas: That first concept is modified so that flooding occurs only within the boundaries of an area, and the resulting link state database contains only information from the routers in the area. This, in turn, means that each router‚Äôs calculated shortest-path tree only describes the path to other routers within the area.\nThe third concept:\nOSPF areas are connected by one or more Area Border Routers (the other main link state protocol, IS-IS, connects areas somewhat differently) which maintain a separate link state database and calculate a separate shortest-path tree for each of their connected areas. So an ABR by definition is a member of two or more areas. It advertises the prefixes it learns in one area to its other areas by flooding Type 3 LSAs into the areas that basically say, ‚ÄúI know how to reach these destinations.‚Äù Wait a minute ‚Äì what that last concept described is not link state, it‚Äôs distance vector. The routers in an area cannot ‚Äúsee‚Äù past the ABR, and rely on the ABR to correctly tell them what prefixes it can reach. The SPF calculation within an area derives a shortest-path tree that depicts all prefixes beyond the ABR as leaf subnets connected to the ABR at some specified cost.\nAnd that leads us to the answer to the question: Because inter-area OSPF is distance vector, it is vulnerable to routing loops. It avoids loops by mandating a loop-free inter-area topology, in which traffic from one area can only reach another area through area 0.\n","permalink":"http://localhost:1313/posts/my-notes-about-ospf/","summary":"\u003ch1 id=\"introduction\"\u003eIntroduction\u003c/h1\u003e\n\u003cp\u003eThis post is a summary of OSPF that I compiled during my CCIE journey, gathering information from RFCs, books, Cisco documentation, blogs, and other sources.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e\u003cem\u003eIMPORTANT:\u003c/em\u003e\u003c/strong\u003e I‚Äôm still in the process of converting this page from my original notes, so some information may be missing, and the formatting may not yet be fully refined.\u003c/p\u003e\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch1 id=\"ospfv2\"\u003eOSPFv2\u003c/h1\u003e\n\u003cp\u003eOSPFv2 is documented under \u003cstrong\u003eRFC 2328\u003c/strong\u003e, which states the following:\u003c/p\u003e\n\u003cp\u003eOSPF routes IP packets based solely on the destination IP address found in the IP packet header.  IP packets are routed \u0026ldquo;as is\u0026rdquo; \u0026ndash; they are not encapsulated in any further protocol headers as they transit the Autonomous System.  OSPF is a dynamic routing protocol. It quickly detects topological changes in the AS (such as router interface failures) and calculates new loop-free routes after a period of convergence.\u003c/p\u003e","title":"My notes about OSPF"},{"content":"In this post we\u0026rsquo;re going to explore some Cisco IOS XE capabilities such as Streaming Telemetry and Guestshell.\nThere is a ton of content available on Cisco DevNet explaining Model-Driven Telemetry theory in great detail, so I highly suggest you take some time to browse through the links I\u0026rsquo;ve listed under the Reference of this post.\nSummary My lab router is a Cisco ISR C1111-4G running IOS XE 17.6.3a. The same environment can be set up on a CSR1000v or Catalyst 8000v running on Cisco CML for example.\nYou will notice I\u0026rsquo;m using subnet 192.168.111.0/24 on an interface named VirtualPortGroup0. This is a virtual interface that will be used for communication between the Guestshell and IOS XE. Feel free to change the IP addressing as needed.\nI needed to NAT overload the above subnet out the router\u0026rsquo;s WAN interface so that Guestshell could to reach the Internet. This is optional and you should adjust as needed, depending on your environment.\nWe will be installing packages on Guestshell\u0026rsquo;s OS using the Yum package manager. For this reason, configuring Guestshell to query a real DNS server is mandatory. I used Google\u0026rsquo;s 8.8.8.8 for this lab.\nNote: Guestshell has limited resources so this kind of setup should be used for testing or labbing purposes only. If you plan on collecting MDT on a production environment, make sure to size your servers accordingly.\nGuestshell First of all, I want to make sure you understand the implications of running a database under Guestshell environment. Guestshell is a Linux container running on top of IOS XE, so you will have limited resources. Depending on the platform you\u0026rsquo;re running, you should be able to increase resources for Guestshell, though I\u0026rsquo;m not going to cover that in this post.\nTo keep it short, if you plan on streaming Model-Driven Telemetry on a production environment, please make sure you host your Collector and Databases on dedicated servers and configure data retention accordingly.\nEnabling IOX service Ok, our first step is to enable the IOX service under IOS XE.\nconf t iox end Verifying IOX status If you just enabled IOX, give it a few minutes and check the services status with the following command:\nRouter#show iox IOx Infrastructure Summary: --------------------------- IOx service (CAF) : Running IOx service (HA) : Not Supported IOx service (IOxman) : Running IOx service (Sec storage) : Not Running Libvirtd 5.5.0 : Running Note: Make sure services CAF, IOxman and Libvirtd are running before proceding to the next steps.\nSetting App-Hosting Parameters Another step before enabling Guestshell is to set the networking configuration the container will use. In my lab I will be using the following settings:\nGuestshell IP: 192.168.111.2/24 Default Gateway: 192.168.111.1 DNS server: 8.8.8.8 Feel free to change the IP addressing as needed, just make sure to use a valid DNS server.\nconf t ! app-hosting appid guestshell app-vnic gateway0 virtualportgroup 0 guest-interface 0 guest-ipaddress 192.168.111.2 netmask 255.255.255.0 app-default-gateway 192.168.111.1 guest-interface 0 name-server0 8.8.8.8 ! end NAT configuration - optional My upstream router (ISP) only supports static routing and, since this is just a lab, I was easier for me to just NAT overload all traffic from the Guestshell towards my LAN instead of creating static routes on my ISP\u0026rsquo;s router. Again, this is an optional step. Just keep in mind that Guestshell will need to reach the Internet.\nIn case you do need to NAT, here\u0026rsquo;s the configuration I\u0026rsquo;m using:\nconf t ! ip access-list extended NAT_ACL 10 permit ip 192.168.111.0 0.0.0.255 any ! ip nat inside source list NAT_ACL interface GigabitEthernet0/0/0 overload ! interface GigabitEthernet0/0/0 description This is my routers WAN interface ip nat outside ! interface VirtualPortGroup0 description This virtual interface will be Guestshells default gateway ip address 192.168.111.1 255.255.255.0 ip nat inside ! end Enabling Guestshell After enabling IOX service and configuring App-Hosting parameters, the next step is to instantiate the Guestshell container:\nRouter#guestshell enable Interface will be selected if configured in app-hosting Please wait for completion Guestshell enabled successfully Guestshell is now up and running, so we can get access to the Linux shell:\nRouter#guestshell run bash [guestshell@guestshell ~]$ Setting up the Collector and Database At this point, we need to install and configure our Collector and our Time Series Database. For this lab I\u0026rsquo;m using Telegraf and InfluxDB, but there are other options out there that will achieve the same results.\nFrom the Guestshell Bash:\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo [influxdb] name = InfluxDB Repository - RHEL \\$releasever baseurl = https://repos.influxdata.com/rhel/\\$releasever/\\$basearch/stable enabled = 1 gpgcheck = 1 gpgkey = https://repos.influxdata.com/influxdata-archive_compat.key EOF Install Telegraf and InfluxDB using Yum:\nsudo yum install telegraf influxdb2 Now let\u0026rsquo;s edit Telegraf\u0026rsquo;s config file. Edit /etc/telegraf/telegraf.conf and uncomment the following lines:\n[[outputs.influxdb_v2]] urls = [\u0026#34;http://127.0.0.1:8086\u0026#34;] # leave the token empty for now token = \u0026#34;\u0026#34; organization = \u0026#34;MDT-LAB\u0026#34; bucket = \u0026#34;telegraf\u0026#34; [[inputs.cisco_telemetry_mdt]] transport = \u0026#34;grpc\u0026#34; service_address = \u0026#34;:57000\u0026#34; max_msg_size = 4000000 Save the file and start the services:\nsudo systemctl start influxdb sudo systemctl start telegraf Now that InfluxDB is running, let\u0026rsquo;s use the browser to open the GUI at http://\u0026lt;your_guestshell_ip\u0026gt;:8086\nClick GET STARTED and (1) set up an account, (2) create a bucket and (3) create an API Token for Telegraf\nClick CONFIGURE LATER\nExpand the left-side menu and click LOAD DATA\nGo to API TOKENS and click GENERATE API TOKEN\nGive it read+write permissions under bucket named telegraf and click GENERATE\nAPI Token will be used on Telegraf config\nNow, we need to go back and edit /etc/telegraf/telegraf.conf once again to include the API token:\n# Token for authentication token = \u0026#34;insert_your_token_here\u0026#34; Save the file and restart Telegraf service:\nsudo systemctl restart telegraf At this point our collector and database are ready to go!\nModel-Driven Telemetry As Cisco states: Model-driven Telemetry (MDT) provides a mechanism to stream data from an MDT-capable device to a destination. The data to be streamed is subscribed from a data set in a YANG model. The data from the subscribed data set is streamed out to the destination either at a configured periodic interval or only when an event occurs.\nOn the next steps we\u0026rsquo;re going to enable NETCONF on IOS XE and also configure our Telemetry Subscriptions that we want to stream to the collector (Telegraf running on Guestshell).\nNETCONF Let\u0026rsquo;s enable NETCONF:\nconf t netconf-yang end Telemetry Subscriptions Still on IOS XE, the next step is to configure the Telemetry Subscriptions. By looking at the configuration below, you will notice we will be using gRPC as the transport protocol.\nThe key here is that YANG XPaths are used to select the information we want to stream. If you want to explore further, I suggest using Cisco YANG Suite (see link at the end of the post).\nCreating Telemetry Subscriptions on IOS XE:\ntelemetry ietf subscription 100 encoding encode-kvgpb filter xpath /process-cpu-ios-xe-oper:cpu-usage/cpu-utilization/five-seconds source-address 192.168.111.1 stream yang-push update-policy periodic 500 receiver ip address 192.168.111.2 57000 protocol grpc-tcp telemetry ietf subscription 101 encoding encode-kvgpb filter xpath /memory-ios-xe-oper:memory-statistics/memory-statistic source-address 192.168.111.1 stream yang-push update-policy periodic 500 receiver ip address 192.168.111.2 57000 protocol grpc-tcp telemetry ietf subscription 200 encoding encode-kvgpb filter xpath /interfaces-ios-xe-oper:interfaces/interface[name=\u0026#39;GigabitEthernet0/0/0\u0026#39;]/statistics source-address 192.168.111.1 stream yang-push update-policy periodic 500 receiver ip address 192.168.111.2 57000 protocol grpc-tcp telemetry ietf subscription 201 encoding encode-kvgpb filter xpath /interfaces-ios-xe-oper:interfaces/interface[name=\u0026#39;GigabitEthernet0/1/1\u0026#39;]/statistics source-address 192.168.111.1 stream yang-push update-policy periodic 500 receiver ip address 192.168.111.2 57000 protocol grpc-tcp telemetry ietf subscription 300 encoding encode-kvgpb filter xpath /interfaces/interface[name=\u0026#39;GigabitEthernet0/0/0\u0026#39;]/diffserv-info/diffserv-target-classifier-stats source-address 192.168.111.1 stream yang-push update-policy periodic 500 receiver ip address 192.168.111.2 57000 protocol grpc-tcp The subscription IDs are arbitrary and locally significant only.\nMost of the XPaths above are self-explanatory and you will probably notice that we will be monitoring the router\u0026rsquo;s CPU, Memory and Statistics from interfaces GigabitEthernet0/0/0 and GigabitEthernet0/1/1. Again, adjust the interfaces naming to match your environment.\nIf you saw my QoS monitoring post on LinkedIn, then take a closer look at the Subscription id 300. Note that /interfaces/interface[name=\u0026lsquo;GigabitEthernet0/0/0\u0026rsquo;]/diffserv-info/diffserv-target-classifier-stats is the XPath related to diffserv classifier statistics for my router\u0026rsquo;s WAN interface.\nCollecting and Storing Telemetry Data After configuring the Telemetry Subscriptions on IOS XE, let\u0026rsquo;s go back to the InfluxDB dashboard and check if data is being received:\nCheck if MDT streams are being received\nNotice that Telegraf is now receiving Telemetry Streams from the router. From this point on, you can create a dashboard on Telegraf or even use Grafana to monitor everything you want.\nSo, going back to my QoS monitoring post, this is the configuration I used to create those nice graphs:\nSelecting the desired data\nOn the 1st filter, search for diffserv On the 2nd filter, search for rate On the 3rd filter there\u0026rsquo;s no need to select anything, unless you want to create a graph with specific traffic classes only (notice on the image above that I didn\u0026rsquo;t select anything there) On the 4rd filter: select traffic direction (qos-inbound or qos-outbound) Click submit and you\u0026rsquo;ll get a live preview of the graph If you want to add this specific graph to a dashboard, just click save as (top right of the screen) and create a new dashboard with your new graph:\nAdding the graph to a new dashboard\nSome of the available options\nDashboard example with inbound and outbound QoS traffic statistics\nConclusion We configured a Telegraf collector and InfluxDB running over a Guestshell container. We also configured IOS XE to stream Model-Driven Telemetry using gRPC to the collector+database.\nReferences Enterprise Streaming Telemetry and You: Getting Started with Model Driven Telemetry: https://blogs.cisco.com/developer/getting-started-with-model-driven-telemetry?ref=guilhermelyra.com\nStreaming Telemetry Quick Start Guide: https://developer.cisco.com/docs/ios-xe/?ref=guilhermelyra.com#!streaming-telemetry-quick-start-guide\nCisco YANG Suite: https://developer.cisco.com/yangsuite/?ref=guilhermelyra.com\n","permalink":"http://localhost:1313/posts/ios-xe-telemetry/","summary":"\u003cp\u003eIn this post we\u0026rsquo;re going to explore some Cisco IOS XE capabilities such as \u003cstrong\u003eStreaming Telemetry\u003c/strong\u003e and \u003cstrong\u003eGuestshell\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThere is a ton of content available on \u003cstrong\u003eCisco DevNet\u003c/strong\u003e explaining \u003cstrong\u003eModel-Driven Telemetry\u003c/strong\u003e theory in great detail, so I highly suggest you take some time to browse through the links I\u0026rsquo;ve listed under the \u003cstrong\u003eReference\u003c/strong\u003e of this post.\u003c/p\u003e\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eMy lab router is a Cisco ISR C1111-4G running IOS XE 17.6.3a. The same environment can be set up on a CSR1000v or Catalyst 8000v running on Cisco CML for example.\u003c/p\u003e","title":"Cisco IOS XE Model-Driven Telemetry"},{"content":"In November 2020 I had the honor of being invited by Cisco Community to present a live event about Cisco SD-WAN (Viptela). The event had more than 300 participants, from 9 different countries.\nFor those who want to know more about this excellent Cisco solution, the recording is available on the link below (presentation language is Brazilian Portuguese).\nWatch the video on YouTube: https://www.youtube.com/watch?v=xtTHjDv1r-M\n","permalink":"http://localhost:1313/posts/cisco-community-sdwan-event/","summary":"\u003cp\u003eIn November 2020 I had the honor of being invited by Cisco Community to present a live event about \u003cstrong\u003eCisco SD-WAN (Viptela)\u003c/strong\u003e. The event had more than 300 participants, from 9 different countries.\u003c/p\u003e\n\u003cp\u003eFor those who want to know more about this excellent Cisco solution, the recording is available on the link below (presentation language is Brazilian Portuguese).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eWatch the video on YouTube:\u003c/strong\u003e \u003ca href=\"https://www.youtube.com/watch?v=xtTHjDv1r-M\"\u003ehttps://www.youtube.com/watch?v=xtTHjDv1r-M\u003c/a\u003e\u003c/p\u003e","title":"Cisco Community Live Event - Cisco SD-WAN"}]